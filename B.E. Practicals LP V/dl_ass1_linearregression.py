# -*- coding: utf-8 -*-
"""DL_Ass1_LinearRegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dOXqfxbC7496ca3YFqH_Cp1TVjLnGRVV
"""

!python --version

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf

# Load dataset (ensure 'BostonHousing.csv' is in working directory)
df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv')
X = df.drop('medv', axis=1).values
y = df['medv'].values

# Split and scale data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Build a simple DNN regressor
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(1)
])
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train and evaluate
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)
loss, mae = model.evaluate(X_test, y_test, verbose=0)
print(f"Test MSE: {loss:.2f}, Test MAE: {mae:.2f}")

# Linear Regression by Using Deep Neural Network
# This script demonstrates how to solve the Boston housing price prediction problem using
# a simple deep neural network (DNN) as a linear regressor. It includes:
# 1. Loading the dataset
# 2. Exploring and preprocessing the data
# 3. Building a neural network model with TensorFlow Keras
# 4. Training and evaluating the model
# 5. Visualizing training history and predictions
# 6. Comparing with traditional linear regression
# 7. Predicting on new data
# 8. Understanding the plots: meaning, importance, and how to read them

# Written for Python 3.11.12 and directly runnable as a standalone Python script.

# 1. Setup and Imports
# If running locally and packages are missing, install them using:
# pip install tensorflow pandas matplotlib scikit-learn seaborn

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split  # splits data into train and test sets
from sklearn.preprocessing import StandardScaler       # standardizes features by removing mean and scaling to unit variance
import tensorflow as tf
from tensorflow.keras import Sequential                # linear stack of layers for neural network
from tensorflow.keras.layers import Dense              # fully connected neural network layer
from tensorflow.keras.optimizers import Adam           # optimizer using adaptive moment estimation
from sklearn.linear_model import LinearRegression      # ordinary least squares Linear Regression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

print("TensorFlow version:", tf.__version__)

# 2. Load the Dataset
url = 'https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv'
df = pd.read_csv(url)
print(df.head(), "\n")

# 3. Exploratory Data Analysis (EDA)
print("Dataset shape:", df.shape)
print(df.describe(), "\n")

# Correlation matrix heatmap
# This heatmap visualizes the pairwise correlation coefficients between features and the target.
# Coefficients range from -1 (perfect negative correlation) to +1 (perfect positive correlation).
# Strong correlations (|r|>0.5) highlight features most strongly linked to price, aiding feature selection.
plt.figure(figsize=(12, 10))
sns.heatmap(df.corr(), annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Feature Correlation Matrix')
plt.show()

# 4. Preprocessing
# Separate features (X) and target (y), split into train/test, then scale features

y = df['medv'].values         # target: median value of owner-occupied homes in $1000s
X = df.drop('medv', axis=1).values  # input features: various housing attributes
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test  = scaler.transform(x_test)
print("Train shape:", x_train.shape)
print("Test shape:", x_test.shape)

# 5. Build the Baseline Neural Network Model
baseline_model = Sequential([
    Dense(64, activation='relu', input_shape=(x_train.shape[1],)),  # relu sets negatives to zero
    Dense(32, activation='relu'),
    Dense(1)  # Output layer for regression: predicts a single continuous value
])
baseline_model.compile(
    optimizer=Adam(learning_rate=0.001),   # adaptive learning rate optimizer
    loss='mse',                            # mean squared error loss
    metrics=['mae']                        # mean absolute error metric
)
baseline_model.summary()

# 6. Train the Baseline Model
history_baseline = baseline_model.fit(
    x_train, y_train,
    validation_split=0.2,
    epochs=100,
    batch_size=32,
    verbose=1
)

# 7. Evaluate Baseline Model
test_loss, test_mae = baseline_model.evaluate(x_test, y_test, verbose=0)
print(f"Baseline Test MSE: {test_loss:.2f}")
print(f"Baseline Test MAE: {test_mae:.2f}\n")

# 8. Visualize Baseline Training History
# Plot training and validation loss & MAE over epochs.
# Interpretation:
#  - Loss curves: track how MSE decreases; if validation loss increases while train loss decreases, model is overfitting.
#  - MAE curves: show average prediction error magnitude.
plt.figure(figsize=(12, 5))
# Loss plot
plt.subplot(1, 2, 1)
plt.plot(history_baseline.history['loss'], label='Train Loss')
plt.plot(history_baseline.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('MSE')
plt.legend()
plt.title('Baseline Model Loss')
# MAE plot
plt.subplot(1, 2, 2)
plt.plot(history_baseline.history['mae'], label='Train MAE')
plt.plot(history_baseline.history['val_mae'], label='Validation MAE')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()
plt.title('Baseline Model MAE')
plt.tight_layout()
plt.show()

# 9. Predictions vs. True Values (Baseline)
# Scatter plot: true prices vs. predicted prices for test set.
# Interpretation:
#  - Points along diagonal indicate perfect predictions.
#  - Systematic deviations show bias; spread indicates variance of errors.
y_pred_baseline = baseline_model.predict(x_test).flatten()
plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_pred_baseline, alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('True Prices ($1000s)')
plt.ylabel('Predicted Prices ($1000s)')
plt.title('Baseline: True vs. Predicted')
plt.show()

# 10. Sample Predictions (Baseline)
print("Baseline Sample predictions:")
for i in range(5):
    print(f"True: {y_test[i]:.2f}, Predicted: {y_pred_baseline[i]:.2f}")

# 11. Traditional Linear Regression Model
lr = LinearRegression()
lr.fit(x_train, y_train)
y_pred_lr = lr.predict(x_test)
rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))
mae_lr  = mean_absolute_error(y_test, y_pred_lr)
r2_lr   = r2_score(y_test, y_pred_lr)
print(f"\nLinear Regression RMSE: {rmse_lr:.2f}")
print(f"Linear Regression MAE: {mae_lr:.2f}")
print(f"Linear Regression R^2 Score: {r2_lr:.2f}\n")

# 12. Deeper Neural Network Model
deep_model = Sequential([
    Dense(128, activation='relu', input_shape=(x_train.shape[1],)),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(16, activation='relu'),
    Dense(1)
])
deep_model.compile(
    optimizer='adam',
    loss='mean_squared_error',
    metrics=['mae']
)
deep_model.summary()

dnn_history = deep_model.fit(
    x_train, y_train,
    validation_split=0.05,
    epochs=100,
    batch_size=32,
    verbose=1
)

# 13. Visualize Deeper Model Training
# Plot DNN loss & MAE curves to assess convergence and overfitting.
plt.figure(figsize=(10, 4))
plt.plot(dnn_history.history['loss'], label='Train Loss')
plt.plot(dnn_history.history['val_loss'], label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('MSE')
plt.title('DNN Model Loss')
plt.legend()
plt.show()

plt.figure(figsize=(10, 4))
plt.plot(dnn_history.history['mae'], label='Train MAE')
plt.plot(dnn_history.history['val_mae'], label='Val MAE')
plt.xlabel('Epoch')
plt.ylabel('Mean Absolute Error')
plt.title('DNN Model MAE')
plt.legend()
plt.show()

# 14. Evaluate Deeper Model
dnn_mse, dnn_mae = deep_model.evaluate(x_test, y_test, verbose=0)
print(f"DNN Test MSE: {dnn_mse:.2f}")
print(f"DNN Test MAE: {dnn_mae:.2f}\n")

# 15. Predict on New Data
new_data = np.array([[0.1, 10.0, 5.0, 0, 0.4, 6.0, 50, 6.0, 1, 400, 20, 300, 10]])
new_scaled = scaler.transform(new_data)
new_pred = deep_model.predict(new_scaled)
print(f"Predicted house price for new data: {new_pred.flatten()[0]:.2f} ($1000s)\n")

# 16. Definitions of Key Terms
# train_test_split: splits arrays into random train and test subsets
# StandardScaler: scales features to zero mean and unit variance
# Dense layer: fully connected neural network layer
# activation='relu': rectified linear unit activation function
# optimizer='adam': optimization algorithm combining RMSProp and momentum
# loss='mean_squared_error': average squared error between predicted and true values
# metrics=['mae']: track mean absolute error during training
# epochs: number of full passes over the training data
# batch_size: number of samples processed before updating model weights
# fit(): trains the model
# evaluate(): assesses model performance on test data
# predict(): generates predictions for input samples
# LinearRegression: linear model fitting via least squares
# mean_squared_error: average of squared differences between true and predicted values
# mean_absolute_error: average of absolute differences between true and predicted values
# r2_score: coefficient of determination, fraction of variance explained by the model
# RMSE: root mean squared error, square root of MSE

# End of script

print("NumPy version:", np.__version__)
print("Pandas version:", pd.__version__)
print("Matplotlib version:", matplotlib.__version__)
print("Seaborn version:", sns.__version__)
print("Scikit-learn version:", sklearn.__version__)
print("TensorFlow version:", tf.__version__)

"""Thought for a couple of seconds


**Theory Behind Linear Regression via Deep Neural Networks**

---

### 1. Introduction

Linear regression is a foundational statistical technique that models the relationship between a set of input features and a continuous target variable. In its simplest form, it assumes a linear combination of inputs:

$$
\hat y = w_1 x_1 + w_2 x_2 + \dots + w_p x_p + b
$$

where $x_i$ are features, $w_i$ their weights, and $b$ the bias term.

A Deep Neural Network (DNN) can be seen as a **generalized** linear regressor: by stacking multiple layers and non‑linear activations, it can learn both linear and complex non‑linear patterns in the data. In practice, you can build a minimal DNN with a single hidden layer (or even zero hidden layers) that reduces to standard linear regression, or deeper networks to capture more intricate relationships.

---

### 2. Why Use a DNN for Regression?

* **Flexibility:** Even a one‑layer “network” with no activation on the final layer implements ordinary least squares regression (when trained with MSE loss).
* **Non‑linearity:** By inserting hidden layers with non‑linear activations (e.g. ReLU, sigmoid), the network can approximate highly non‑linear mappings.
* **Scalability:** Modern frameworks like TensorFlow/Keras optimize large‑scale models efficiently with GPUs.
* **Unified API:** You use the same model-building, training, and evaluation workflows whether you’re solving a regression or classification problem.

---

### 3. Dataset: Boston Housing Prices

* **Features (13 total):** e.g. `CRIM` (crime rate), `ZN` (residential land zoned), …, `LSTAT` (% lower status population)
* **Target:** `MEDV` — median value of owner‑occupied homes (in \$1000s).

---

### 4. Key Steps & Syntax

#### 4.1. Environment Setup

```bash
pip install pandas scikit-learn tensorflow
```

#### 4.2. Imports

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
```

#### 4.3. Loading the Data

```python
# Assumes BostonHousing.csv is in your working directory
df = pd.read_csv('BostonHousing.csv')
X = df.drop('medv', axis=1).values   # features
y = df['medv'].values                # target
```

#### 4.4. Train/Test Split & Scaling

```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test  = scaler.transform(X_test)
```

* **Why scale?** Features with different ranges can slow convergence; standardizing to zero mean and unit variance helps gradient‑based optimizers.

#### 4.5. Building the DNN Model

```python
model = tf.keras.Sequential([
    # Hidden layer with ReLU activation
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    # Output layer for regression (no activation)
    tf.keras.layers.Dense(1)
])
model.compile(
    optimizer='adam',        # Adam optimizer adapts learning rate per-parameter
    loss='mean_squared_error',  # MSE suited for regression tasks
    metrics=['mae']             # Track Mean Absolute Error during training
)
```

* **Layers:**

  * `Dense(64, activation='relu')` learns 64 intermediate features.
  * `Dense(1)` produces a single continuous prediction.

#### 4.6. Training

```python
history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=50,
    batch_size=32,
    verbose=1
)
```

* **Epochs:** Full passes over the training data.
* **Batch size:** Number of samples per gradient update.
* **Validation split:** Holds out 20% of train set to monitor over‑/under‑fitting.

#### 4.7. Evaluation

```python
loss, mae = model.evaluate(X_test, y_test, verbose=0)
print(f"Test MSE: {loss:.2f}, Test MAE: {mae:.2f}")
```

* **MSE (Mean Squared Error):** Average squared difference between true and predicted values; sensitive to large errors.
* **MAE (Mean Absolute Error):** Average absolute difference; more interpretable in original units (\$1000s).

#### 4.8. Making Predictions

```python
sample = X_test[0].reshape(1, -1)
predicted_price = model.predict(sample).flatten()[0]
```

* Reshape a single sample to shape `(1, n_features)` before prediction.

---

### 5. From Linear to Deep

* **Zero hidden layers + linear output:** exactly linear regression.
* **One hidden layer (ReLU) + linear output:** a “shallow” neural regressor; can model piecewise linear functions.
* **Multiple hidden layers:** deeper networks capture hierarchical, non‑linear relationships.

---

### 6. Interpreting Results

* **Training curves (loss & MAE vs. epochs):**

  * If **validation loss** plateaus or rises while **training loss** decreases, you’re overfitting.
  * If both curves are high and flat, you’re underfitting (model too simple).
* **Scatter of true vs. predicted:**

  * Ideal: points lie on the 45° line.
  * Systematic deviation indicates bias; scatter around the line indicates variance/noise.

---

### 7. Conclusion

Using a deep neural network for what is traditionally a linear regression problem shows the versatility of the DNN framework. With minimal code changes you can extend from a straight line fit to highly flexible, non‑linear regressors simply by adding layers or changing activations—while still leveraging robust training algorithms (e.g. Adam) and standardized preprocessing steps (e.g. scaling). This unified approach simplifies experimentation when tackling real‑world datasets like Boston Housing prices.

"""

